post_prob
image.plot(prior_as, prior_bs,post_prob,xlab="n0",ylab="theta0",
main="p(theta > 0.5 | y)",col=heat.colors(100)[1:90])
image.plot(matrix_theta, matrix_n0,post_prob,xlab="n0",ylab="theta0",
main="p(theta > 0.5 | y)",col=heat.colors(100)[1:90])
image.plot(matrix_n0, matrix_theta,post_prob,xlab="n0",ylab="theta0",
main="p(theta > 0.5 | y)",col=heat.colors(100)[1:90])
## 3D and 2D plots of the posterior probability that theta > some value w given the data in the
## Beta-Binomial model
## The function that makes 3D plot is the persp function
## For help and documentation, type ?persp at the prompt in R
## The function persp takes as arguments: a x vector that gives the values on the x axis (this has
## to be in increasing order), a y vector with values on the y axis (also in increasing order),
## and a z matrix of dimension equal to the length of the x vector times the length of the y vector.
## The z matrix has at the (i,j) entry the value of the variable to be displayed on the third dimension
## corresponding to the i-th value of the x-vector and the j-th value of the y-vector, that is z[i,j] is
## the value of the z variable at x[i] and y[j].
a <- seq(0,10,by=0.1)
b <- seq(0,10,by=0.1)
w <- 0.8
n <- 129
y <- 118
## Plotting the posterior distribution
#for(i in 1:length(a)){
#	for(j in 1:length(b)){
i=1
j=1
plot(seq(0,1,by=0.01),dbeta(seq(0,1,by=0.01),a[i]+y,b[j]+(n-y)),type="l",col="black",lty=1,lwd=1,
xlim=c(0,1),xlab="Theta",ylab="P(theta | y)")
image.plot(a,b,post.prob.theta,xlab="a",ylab="b",main="p(theta > 0.8 | y)",col=heat.colors(100)[1:90])
## The function that makes 3D plot is the persp function
## For help and documentation, type ?persp at the prompt in R
## The function persp takes as arguments: a x vector that gives the values on the x axis (this has
## to be in increasing order), a y vector with values on the y axis (also in increasing order),
## and a z matrix of dimension equal to the length of the x vector times the length of the y vector.
## The z matrix has at the (i,j) entry the value of the variable to be displayed on the third dimension
## corresponding to the i-th value of the x-vector and the j-th value of the y-vector, that is z[i,j] is
## the value of the z variable at x[i] and y[j].
a <- seq(0,10,by=0.1)
b <- seq(0,10,by=0.1)
w <- 0.8
n <- 129
y <- 118
## Plotting the posterior distribution
#for(i in 1:length(a)){
#	for(j in 1:length(b)){
i=1
j=1
plot(seq(0,1,by=0.01),dbeta(seq(0,1,by=0.01),a[i]+y,b[j]+(n-y)),type="l",col="black",lty=1,lwd=1,
xlim=c(0,1),xlab="Theta",ylab="P(theta | y)")
title(main=paste("Posterior distribution a=",a[i]," b=",b[j],sep=""))
abline(v=0.8,lty=3,col="red")
#	}
#}
post.prob.theta <- matrix(0,length(a),length(b))
for(i in 1:length(a)){
for(j in 1:length(b)){
post.a <- a[i]+y
post.b <- b[j]+(n-y)
post.prob.theta[i,j] <- 1-pbeta(w,post.a,post.b)
}
}
persp(a,b,post.prob.theta,xlab="a",ylab="b",zlab="p(theta > w | y)")
# To rotate the figure, you can specify the value of the angles theta and phi by which to tilt the figure
persp(a,b,post.prob.theta,xlab="a",ylab="b",zlab="p(theta > 0.8 | y)",main="P(theta > 0.8 | y)",
theta=30,phi=20) # try ticktype = "detailed"
##### The 2D color plot may be better than the 3D plot.
## To make these 2D color plots it is necessary to download the R package fields
## If you do not know how to do this, please look at the reference R manual that is in th CTools website
## Once the package has been downloaded, you can install it by typing the command library(fields) at the prompt
library(fields)
## The function that makes 2D plot is the image.plot function
## For help and documentation, type ?image.plot at the prompt in R
## As the function persp, the function image.plot takes as arguments: a x vector that gives the values on the x axis (this has
## to be in increasing order), a y vector with values on the y axis (also in increasing order),
## and a z matrix of dimension equal to the length of the x vector times the length of the y vector.
## The z matrix has at the (i,j) entry the value of the variable to be displayed using color codes
## corresponding to the i-th value of the x-vector and the j-th value of the y-vector.
a <- seq(0.1,10,by=0.1)
b <- seq(0.1,10,by=0.1)
w <- 0.8
n <- 129
y <- 118
post.prob.theta <- matrix(0,length(a),length(b))
post.mean.theta <- matrix(0,length(a),length(b))
theta0 <- matrix(0,length(a),length(b))
n0 <- matrix(0,length(a),length(b))
for(i in 1:length(a)){
for(j in 1:length(b)){
post.a <- a[i]+y
post.b <- b[j]+(n-y)
post.prob.theta[i,j] <- 1-pbeta(w,post.a,post.b)
post.mean.theta[i,j] <- post.a/(post.a+post.b)
theta0[i,j] <- a[i]/(a[i]+b[j])
n0[i,j] <- a[i]+b[j]
}
}
image.plot(a,b,post.prob.theta,xlab="a",ylab="b",main="p(theta > 0.8 | y)",col=heat.colors(100)[1:90])
post.prob.theta
image.plot(n0,theta0,post.prob.theta,xlab="n0",ylab="theta0",main="p(theta > 0.8 | y)",col=heat.colors(100)[1:90])
b
image.plot(n0s, theta0s,post_prob,xlab="n0",ylab="theta0",
main="p(theta > 0.5 | y)",col=heat.colors(100)[1:90])
dim(post_prob)
length(n0s)
length(theta0s)
image.plot(n0s, theta0s,t(post_prob),xlab="n0",ylab="theta0",
main="p(theta > 0.5 | y)",col=heat.colors(100)[1:90])
image.plot(a,b,post.prob.theta,xlab="a",ylab="b",main="p(theta > 0.8 | y)",col=heat.colors(100)[1:90])
image.plot(a,b,post.prob.theta,xlab="a",ylab="b",main="p(theta > 0.8 | y)",col=heat.colors(100)[1:90])
## 3D and 2D plots of the posterior probability that theta > some value w given the data in the
## Beta-Binomial model
## The function that makes 3D plot is the persp function
## For help and documentation, type ?persp at the prompt in R
## The function persp takes as arguments: a x vector that gives the values on the x axis (this has
## to be in increasing order), a y vector with values on the y axis (also in increasing order),
## and a z matrix of dimension equal to the length of the x vector times the length of the y vector.
## The z matrix has at the (i,j) entry the value of the variable to be displayed on the third dimension
## corresponding to the i-th value of the x-vector and the j-th value of the y-vector, that is z[i,j] is
## the value of the z variable at x[i] and y[j].
a <- seq(0,10,by=0.1)
b <- seq(0,10,by=0.1)
w <- 0.8
n <- 129
y <- 118
## Plotting the posterior distribution
#for(i in 1:length(a)){
#	for(j in 1:length(b)){
i=1
j=1
plot(seq(0,1,by=0.01),dbeta(seq(0,1,by=0.01),a[i]+y,b[j]+(n-y)),type="l",col="black",lty=1,lwd=1,
xlim=c(0,1),xlab="Theta",ylab="P(theta | y)")
image.plot(n0,theta0,post.prob.theta,xlab="n0",ylab="theta0",main="p(theta > 0.8 | y)",col=heat.colors(100)[1:90])
image.plot(a,b,post.prob.theta,xlab="a",ylab="b",main="p(theta > 0.8 | y)",col=heat.colors(100)[1:90])
prior_as
prior_bs
theta0s = seq(0.1,0.9, 0.1)
n0s = c(1,2,8,16,32)
w = 0.5
n = 100
y = 57
prior_as <- n0s %*% t(theta0s)
prior_bs <- n0s %*% t((1-theta0s))
post_prob <- matrix(nrow = length(n0s), ncol = length(theta0s))
post_a <- prior_as + y
post_b <- prior_bs + n - y
for (i in 1:length(n0s)){
for (j in 1:length(theta0s)){
post_prob[i,j] <- 1-pbeta(w,post_a[i,j],post_b[i,j])
}
}
image.plot(n0s, theta0s,post_prob,xlab="n0",ylab="theta0",
main="p(theta > 0.5 | y)",col=heat.colors(100)[1:90])
image.plot(n0s, theta0s,post_prob,xlab="n0",ylab="theta0",
main="p(theta > 0.5 | y)",col=heat.colors(100)[1:90])
image.plot(prior_as, prior_bs,post_prob,xlab="a",ylab="b",
main="p(theta > 0.5 | y)",col=heat.colors(100)[1:90])
theta0s = seq(0.1,0.9, 0.01)
n0s = c(1,2,8,16,32)
w = 0.5
n = 100
y = 57
prior_as <- n0s %*% t(theta0s)
prior_bs <- n0s %*% t((1-theta0s))
post_prob <- matrix(nrow = length(n0s), ncol = length(theta0s))
post_a <- prior_as + y
post_b <- prior_bs + n - y
for (i in 1:length(n0s)){
for (j in 1:length(theta0s)){
post_prob[i,j] <- 1-pbeta(w,post_a[i,j],post_b[i,j])
}
}
image.plot(n0s, theta0s,post_prob,xlab="n0",ylab="theta0",
main="p(theta > 0.5 | y)",col=heat.colors(100)[1:90])
image.plot(prior_as, prior_bs,post_prob,xlab="a",ylab="b",
main="p(theta > 0.5 | y)",col=heat.colors(100)[1:90])
theta0s = seq(0.1,0.9, 0.1)
n0s = c(1,2,8,16,32)
w = 0.5
n = 100
y = 57
prior_as <- n0s %*% t(theta0s)
prior_bs <- n0s %*% t((1-theta0s))
post_prob <- matrix(nrow = length(n0s), ncol = length(theta0s))
post_a <- prior_as + y
post_b <- prior_bs + n - y
for (i in 1:length(n0s)){
for (j in 1:length(theta0s)){
post_prob[i,j] <- 1-pbeta(w,post_a[i,j],post_b[i,j])
}
}
image.plot(n0s, theta0s,post_prob,xlab="n0",ylab="theta0",
main="p(theta > 0.5 | y)",col=heat.colors(100)[1:90])
image.plot(prior_as, prior_bs,post_prob,xlab="a",ylab="b",
main="p(theta > 0.5 | y)",col=heat.colors(100)[1:90])
theta0s = seq(0.1,0.9, 0.1)
n0s = c(1,2,8,16,32)
w = 0.5
n = 100
y = 57
prior_as <- n0s %*% t(theta0s)
prior_bs <- n0s %*% t((1-theta0s))
post_prob <- matrix(nrow = length(n0s), ncol = length(theta0s))
post_a <- prior_as + y
post_b <- prior_bs + n - y
for (i in 1:length(n0s)){
for (j in 1:length(theta0s)){
post_prob[i,j] <- 1-pbeta(w,post_a[i,j],post_b[i,j])
}
}
image.plot(n0s, theta0s,post_prob,xlab="n0",ylab="theta0",
main="p(theta > 0.5 | y)",col=heat.colors(100)[90:1])
image.plot(prior_as, prior_bs,post_prob,xlab="a",ylab="b",
main="p(theta > 0.5 | y)",col=heat.colors(100)[90:1])
tinytex::install_tinytex()
knitr::opts_chunk$set(echo = TRUE)
# library(npcs)
library(tidyverse)
library(caret)
library(dfoptim)     # to avoid error: can't find function
sapply(list.files(pattern = "\\.R$"), source)   # source all R file in npcs
# loadhistory(file = ".Rhistory")
# generate simulation data & make sure the replicate np_edited function works fine
set.seed(10, kind = "L'Ecuyer-CMRG")
train <- generate_data(1000, model.no = 1)
set.seed(1, kind = "L'Ecuyer-CMRG")
test <- generate_data(2000, model.no = 1)
# modeling function test:
x1 <- modeling(data = data.frame(x,y=y), classifier = "logistic", trControl = list(), tuneGrid = list())
x1 <- npcs(x = train$x, y = train$y, algorithm ="CX", classifier = "logistic", w=c(1,0,0), alpha=c(NA,0.05, 0.2),
trControl = list(method="cv", number=3))
x1 <- npcs(x = train$x, y = train$y, algorithm ="CX", classifier = "logistic", w=c(1,0,0), alpha=c(NA,0.1, 0.2),
trControl = list(method="cv", number=3))
x1
npcs(x = train$x, y = train$y, algorithm ="ER", classifier = "knn", w=c(1,0,0), alpha=c(NA,0.05, 0.2),
trControl = list(method="cv", number=3), tuneGrid = list(k=c(1,5,10,20)))
modeling <- function(data, classifier, trControl, tuneGrid){
# ---
# :params data:
# :params trControl: list, inputs for resampling in caret::train; default method="none"
# :params tuneGrid: list, inputs for hyperparameters tuning in caret::train()
# if not customized -> use the default trControl & tuneGrid -> may be very slow
# ---
# trainControl inputs:
default_trControl <- list(method="none")
default_tuneGrid <- list()
param_names <- union(names(trControl), names(default_trControl))
for (nm in param_names){  # if arg doesn't exist in input, use default
v1 <- trControl[[nm]]
trControl[[nm]] <- ifelse(is.null(v1), default_trControl[[nm]], v1)
}
trControl <- do.call(trainControl,trControl)
# tuneGrid inputs:
if (classifier=="xgbTree"){
default_tuneGrid <- list(nrounds = 100,  # modify hardcoding in caret to build model only once
max_depth = 1,
eta = .1,
gamma = 0,
colsample_bytree = .7,
min_child_weight = 1,
subsample = .8)
} else if (classifier=="logistic"){
default_tuneGrid <- list(alpha=0,lambda=0)
} else if (classifier=="nb"){
default_tuneGrid <- list(usekernel = TRUE, fL = 0, adjust = 1)
} else if (classifier=="nnb"){
default_tuneGrid <- list(usekernel = FALSE, fL = 0, adjust = 1)
}
param_names <- union(names(tuneGrid), names(default_tuneGrid))
for (nm in param_names){  # if arg doesn't exist in input, use default
v1 <- tuneGrid[[nm]]
tuneGrid[[nm]] <- ifelse(is.null(v1), default_tuneGrid[[nm]], v1)
}
tuneGrid <- do.call(expand.grid, tuneGrid)
if (length(tuneGrid)==0){
if (classifier=="gbm"){
fit <- train(y~.,data=data, method="gbm", trControl=trControl, verbose=F)
} else {
fit <- train(y~.,data=data, method=classifier, trControl=trControl)
}
} else {
if (classifier=="xgbTree"){
fit <- train(y~.,data=data, method="xgbTree", trControl=trControl, tuneGrid=tuneGrid, verbosity=0)
} else if (classifier=="gbm"){
fit <- train(y~.,data=data, method="gbm", trControl=trControl, tuneGrid=tuneGrid, verbose=F)
} else if (classifier=="logistic"){
fit <- train(as.factor(y)~.,data=data, method="glmnet", trControl=trControl, tuneGrid=tuneGrid)
} else if (classifier=="nnb"){
fit <- train(y~.,data=data, method="nb", trControl=trControl, tuneGrid=tuneGrid)
}
else {
fit <- train(y~.,data=data, method=classifier, trControl=trControl, tuneGrid=tuneGrid)
}
}
return(fit)
}
npcs(x = train$x, y = train$y, algorithm ="ER", classifier = "knn", w=c(1,0,0), alpha=c(NA,0.05, 0.2),
trControl = list(method="cv", number=3), tuneGrid = list(k=c(1,5,10,20)))
npcs(x = train$x, y = train$y, algorithm ="ER", classifier = "knn", w=c(1,0,0), alpha=c(NA,0.1, 0.2),
trControl = list(method="cv", number=3), tuneGrid = list(k=c(1,5,10,20)))
npcs(x = train$x, y = train$y, algorithm ="ER", classifier = "knn", w=c(1,0,0), alpha=c(NA,0.2, 0.2),
trControl = list(method="cv", number=3), tuneGrid = list(k=c(1,5,10,20)))
x1 <- npcs(x = train$x, y = train$y, algorithm ="ER", classifier = "knn", w=c(1,0,0), alpha=c(NA,0.2, 0.2),
trControl = list(method="cv", number=3), tuneGrid = list(k=c(1,5,10,20)))
x1 <- npcs(x = train$x, y = train$y, algorithm ="ER", classifier = "knn", w=c(1,0,0), alpha=c(NA,0.2, 0.2),
trControl = list(method="cv", number=3), tuneGrid = list(k=c(1,5,10,20)))
error_rate(x1)
error_rate(x1, train$y)
error_rate(x1, train$x, train$y)
x1 <- npcs(x = train$x, y = train$y, algorithm ="CX", classifier = "knn", w=c(1,0,0), alpha=c(NA,0.2, 0.2),
trControl = list(method="cv", number=3), tuneGrid = list(k=c(1,5,10,20)))
x1
# modeling function test:
x1 <- modeling(data = data.frame(train$x,y=y), classifier = "knn", trControl = list(), tuneGrid = list(k=c(1,5,10,20)))
x1
# modeling function test:
x1 <- modeling(data = data.frame(train$x,y=y), classifier = "knn", trControl = list(), tuneGrid = list(k=c(1,5,10,20)))
x1
x1$bestTune
data = data.frame(train$x,y=y); classifier = "knn"; trControl = list(); tuneGrid = list(k=c(1,5,10,20))
# ---
# :params data:
# :params trControl: list, inputs for resampling in caret::train; default method="none"
# :params tuneGrid: list, inputs for hyperparameters tuning in caret::train()
# if not customized -> use the default trControl & tuneGrid -> may be very slow
# ---
# trainControl inputs:
default_trControl <- list(method="none")
default_tuneGrid <- list()
param_names <- union(names(trControl), names(default_trControl))
for (nm in param_names){  # if arg doesn't exist in input, use default
v1 <- trControl[[nm]]
trControl[[nm]] <- ifelse(is.null(v1), default_trControl[[nm]], v1)
}
trControl <- do.call(trainControl,trControl)
names(tuneGrid)
names(default_tuneGrid)
param_names <- union(names(tuneGrid), names(default_tuneGrid))
param_names
for (nm in param_names){  # if arg doesn't exist in input, use default
v1 <- tuneGrid[[nm]]
tuneGrid[[nm]] <- ifelse(is.null(v1), default_tuneGrid[[nm]], v1)
}
tuneGrid
tuneGrid[[k]]
tuneGrid[['k']]
tuneGrid = list(k=c(1,2,5,10,20))
tuneGrid
tuneGrid[["k"]]
modeling <- function(data, classifier, trControl, tuneGrid){
# ---
# :params data:
# :params trControl: list, inputs for resampling in caret::train; default method="none"
# :params tuneGrid: list, inputs for hyperparameters tuning in caret::train()
# if not customized -> use the default trControl & tuneGrid -> may be very slow
# ---
# trainControl inputs:
default_trControl <- list(method="none")
default_tuneGrid <- list()
param_names <- union(names(trControl), names(default_trControl))
for (nm in param_names){  # if arg doesn't exist in input, use default
v1 <- trControl[[nm]]
if (is.null(v1)){
trControl[[nm]] <- v1
} else {
tuneGrid[[nm]] <- default_trControl[[nm]]
}
}
trControl <- do.call(trainControl,trControl)
# tuneGrid inputs:
if (classifier=="xgbTree"){
default_tuneGrid <- list(nrounds = 100,  # modify hardcoding in caret to build model only once
max_depth = 1,
eta = .1,
gamma = 0,
colsample_bytree = .7,
min_child_weight = 1,
subsample = .8)
} else if (classifier=="logistic"){
default_tuneGrid <- list(alpha=0,lambda=0)
} else if (classifier=="nb"){
default_tuneGrid <- list(usekernel = TRUE, fL = 0, adjust = 1)
} else if (classifier=="nnb"){
default_tuneGrid <- list(usekernel = FALSE, fL = 0, adjust = 1)
}
param_names <- union(names(tuneGrid), names(default_tuneGrid))
for (nm in param_names){  # if arg doesn't exist in input, use default
v1 <- tuneGrid[[nm]]
if (is.null(v1)){
tuneGrid[[nm]] <- v1
} else {
tuneGrid[[nm]] <- default_tuneGrid[[nm]]
}
}
tuneGrid <- do.call(expand.grid, tuneGrid)
if (length(tuneGrid)==0){
if (classifier=="gbm"){
fit <- train(y~.,data=data, method="gbm", trControl=trControl, verbose=F)
} else {
fit <- train(y~.,data=data, method=classifier, trControl=trControl)
}
} else {
if (classifier=="xgbTree"){
fit <- train(y~.,data=data, method="xgbTree", trControl=trControl, tuneGrid=tuneGrid, verbosity=0)
} else if (classifier=="gbm"){
fit <- train(y~.,data=data, method="gbm", trControl=trControl, tuneGrid=tuneGrid, verbose=F)
} else if (classifier=="logistic"){
fit <- train(as.factor(y)~.,data=data, method="glmnet", trControl=trControl, tuneGrid=tuneGrid)
} else if (classifier=="nnb"){
fit <- train(y~.,data=data, method="nb", trControl=trControl, tuneGrid=tuneGrid)
}
else {
fit <- train(y~.,data=data, method=classifier, trControl=trControl, tuneGrid=tuneGrid)
}
}
return(fit)
}
x1 <- modeling(data = data.frame(train$x,y=y), classifier = "knn", trControl = list(), tuneGrid = list(k=c(1,5,10,20)))
x1
y
x1 <- modeling(data = data.frame(train$x,y=train$y), classifier = "knn", trControl = list(), tuneGrid = list(k=c(1,5,10,20)))
x1$bestTune
x1
x1 <- npcs(x = train$x, y = train$y, algorithm ="CX", classifier = "knn", w=c(1,0,0), alpha=c(NA,0.2, 0.2),
trControl = list(method="cv", number=3), tuneGrid = list(k=c(1,5,10,20)))
x1
predict(x1)
predict(x1, newx = train$x)
predict(x1, newx = data.frame(train$x))
data.frame(train$x)
predict(x1, newx = data.frame(train$x), type="raw")
error_rate(predict(x1, newx = data.frame(train$x), type="raw"), train$y)
knitr::opts_chunk$set(echo = TRUE)
list.files(pattern = "*.R")
sapply(list.files(pattern = "*.R"), source,.GlobalEnv)
library(tidyverse)
library(caret)
library(dfoptim)
list.files(pattern = "*.R")
sapply(list.files(pattern = "*.R"), source)
sapply(list.files(pattern = "\\.R$"), source)
library(tidyverse)
library(caret)
library(dfoptim)
sapply(list.files(pattern = "\\.R$"), source)
x1$fit
knn3(y~., data = data.frame(train$x, y=y))
ori <- knn3(y~., data = data.frame(train$x, y=y))
predict(ori, newx = data.frame(train$x))
data.frame(train$x)
ori
predict(ori, newx = train$x)
data.frame(train$x, y=y)
ori <- knn3(y~., data = data.frame(train$x, y=y))
ori <- knn3(y~., data = data.frame(train$x, y=y))
summary(knn3)
ori
summary(ori)
print(ori)
ori$xlevels
data.frame(train$x, y=y)
ori <- knn3(y~X1+X2+X3+X4+X5, data = data.frame(train$x, y=y))
predict(ori, newx = train$x)
predict(ori, newx = data.frame(train$x))
ori
predict(ori)
data.frame(train$x, y=y)
trControl <- list(method="cv", number=3)
tuneGrid <- list(k=c(1,5,10,20))
x1 <- npcs(x = train$x, y = train$y, algorithm ="CX", classifier = "knn", w=c(1,0,0), alpha=c(NA,0.2, 0.2),
trControl=trControl, tuneGrid=tuneGrid)
ori <- modeling(data = data.frame(x, y=y), classifier="knn", trControl=trControl, tuneGrid=tuneGrid)
ori <- modeling(data = data.frame(train$x, y=train$y), classifier="knn", trControl=trControl, tuneGrid=tuneGrid)
predict(ori)
ori_pred <- predict(ori, newx = data.frame(test$x))
ori_pred <- predict(ori, newx = test$x)
test$x
ori_pred <- predict(ori, newx = test$x, type="raw")
ori_pred
np_pred <- predict(x1, newx = test$x)
np_pred <- predict(x1, newx = data.frame(test$x), type="raw")
np_pred
error_rate(ori_pred, test$y)
error_rate(np_pred, test$y)
trControl <- list(method="cv", number=3)
tuneGrid <- list(k=c(1,5,10,20))
x1 <- npcs(x = train$x, y = train$y, algorithm ="CX", classifier = "knn", w=c(1,0,0), alpha=c(NA,0.2, 0.2),
trControl=trControl, tuneGrid=tuneGrid)
ori <- modeling(data = data.frame(train$x, y=train$y), classifier="knn", trControl=trControl, tuneGrid=tuneGrid)
np_pred <- predict(x1, newx = data.frame(test$x), type="raw")
ori_pred <- predict(ori, newx = data.frame(test$x), type="raw")
error_rate(ori_pred, test$y)
error_rate(np_pred, test$y)
ori_pred
ori <- modeling(data = data.frame(train$x, y=train$y), classifier="knn", trControl=trControl, tuneGrid=tuneGrid)
ori
error_rate(ori_pred, test$y)
error_rate(np_pred, test$y)
ori_pred
np_pred
test$y
ori_pred
mean(ori_pred != test$y)
ori
length(tuneGrid)==0
data
ori <- modeling(data = data.frame(train$x, y=train$y), classifier="knn", trControl=trControl, tuneGrid=tuneGrid)
ori_pred <- predict(ori, newx = data.frame(test$x), type="raw")
mean(ori_pred != test$y)
train$y
data
data.frame(train$x, y=train$y)
